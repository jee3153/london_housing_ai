{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b5f7d2-9e71-4c58-afb6-15ba41bc7f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nour EDA should:\\n- explore one dataset\\n- understand its distributions\\n- understand missingness\\n- detect outliers\\n- evaluate train/validation drift\\nYour Data Quality page will show per-run dataset quality.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\"\"\"\n",
    "our EDA should:\n",
    "- explore one dataset\n",
    "- understand its distributions\n",
    "- understand missingness\n",
    "- detect outliers\n",
    "- evaluate train/validation drift\n",
    "Your Data Quality page will show per-run dataset quality.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c18061-533f-4c1f-9c7b-8a2914cd3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1073649, 16)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path(\"/Users/eunjee.yang/code/ai_platform\")\n",
    "csv_path = repo_root / \"backend\" / \"src\" / \"london_housing_ai\" / \"data\" / \"Property_Prices_2022.csv\"\n",
    "\n",
    "cols = [\n",
    "    \"transaction_id\",\n",
    "    \"price\",\n",
    "    \"date\",\n",
    "    \"postcode\",\n",
    "    \"property_type\",\n",
    "    \"old_new\",\n",
    "    \"duration\",\n",
    "    \"paon\",\n",
    "    \"saon\",\n",
    "    \"street\",\n",
    "    \"locality\",\n",
    "    \"town_city\",\n",
    "    \"district\",\n",
    "    \"county\",\n",
    "    \"ppdcategory_type\",\n",
    "    \"record_status\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    csv_path,\n",
    "    header=None,   # because the file has no header row\n",
    "    names=cols,\n",
    ")\n",
    "print(df.shape) # (# rows, # cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1639d6-abec-4b0b-933a-18de53018692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saon                0.856963\n",
      "locality            0.622550\n",
      "street              0.016961\n",
      "postcode            0.002791\n",
      "transaction_id      0.000000\n",
      "price               0.000000\n",
      "date                0.000000\n",
      "property_type       0.000000\n",
      "old_new             0.000000\n",
      "duration            0.000000\n",
      "paon                0.000000\n",
      "town_city           0.000000\n",
      "district            0.000000\n",
      "county              0.000000\n",
      "ppdcategory_type    0.000000\n",
      "record_status       0.000000\n",
      "dtype: float64\n",
      "{'transaction_id': 0, 'price': 0, 'date': 0, 'postcode': 2997, 'property_type': 0, 'old_new': 0, 'duration': 0, 'paon': 0, 'saon': 920078, 'street': 18210, 'locality': 668400, 'town_city': 0, 'district': 0, 'county': 0, 'ppdcategory_type': 0, 'record_status': 0}\n"
     ]
    }
   ],
   "source": [
    "# missingness per column\n",
    "\n",
    "\n",
    "missing = df.isna().mean().sort_values(ascending=False)\n",
    "missing_count = df.isna().sum()\n",
    "print(missing)\n",
    "print(missing_count.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f356de73-4c38-4d9c-a36f-19e4914249f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transaction_id      object\n",
      "price                int64\n",
      "date                object\n",
      "postcode            object\n",
      "property_type       object\n",
      "old_new             object\n",
      "duration            object\n",
      "paon                object\n",
      "saon                object\n",
      "street              object\n",
      "locality            object\n",
      "town_city           object\n",
      "district            object\n",
      "county              object\n",
      "ppdcategory_type    object\n",
      "record_status       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# schema summary\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1225f8d-0b3b-409f-b985-d5dd1a311092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'column': 'price', 'count': np.float64(1073649.0), 'mean': np.float64(413998.0967643988), 'std': np.float64(1758210.9847137122), 'min': np.float64(1.0), '25%': np.float64(179999.0), '50%': np.float64(280000.0), '75%': np.float64(430000.0), 'max': np.float64(480000000.0)}]\n"
     ]
    }
   ],
   "source": [
    "# numeric stats - useful for a \"distribution summary\"\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols].describe().astype(float)\n",
    "stats = []\n",
    "for col in numeric_cols:\n",
    "    stat = { \"column\": col, }\n",
    "    series = df[col].describe().astype(float)\n",
    "    for index, value in zip(series.index, series.values):\n",
    "        stat[index] = value\n",
    "    stats.append(stat)        \n",
    "print(stats)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4157d0cb-06f4-4e7b-a3dd-1d8494d6d682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'price': np.int64(69088)}\n"
     ]
    }
   ],
   "source": [
    "# outlier counts (IQR method)\n",
    "\n",
    "def count_outliers(df, col):\n",
    "    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    return ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "\n",
    "outliers = {col: count_outliers(df, col) for col in numeric_cols}\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700829fd-6ad9-4533-960e-2b682f799621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a3bebea-0035-41d2-b347-6f74a53f62a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': np.float64(0.0021590078652768963)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â drift between train and validation\n",
    "\"\"\"\n",
    "useful for \n",
    "- explainint under/overfitting\n",
    "- understanding poor RMSE\n",
    "- checking validation set is representative\n",
    "\"\"\"\n",
    "\n",
    "train_val_drift = {\n",
    "    col: ks_2samp(train_df[col].dropna(), val_df[col].dropna()).statistic\n",
    "    for col in numeric_cols\n",
    "}    \n",
    "train_val_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a83aee-97d4-4567-9eec-196202b4f2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['property_type', 'old_new', 'duration', 'ppdcategory_type', 'record_status']\n",
      "{'property_type': {'T': 0.26831860319340867, 'S': 0.25901574909490904, 'D': 0.22326849836399046, 'F': 0.1934198234246015, 'O': 0.05597732592309032}, 'old_new': {'N': 0.8775223560027532, 'Y': 0.12247764399724677}, 'duration': {'F': 0.7525066385755493, 'L': 0.24749336142445064}, 'ppdcategory_type': {'A': 0.8456981751019188, 'B': 0.15430182489808122}, 'record_status': {'A': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# Category distribution (for important categorical features)\n",
    "def get_categorical_columns(df, max_unique=50):\n",
    "    \"\"\"Return categorical-like columns (object dtype and low cardinality).\"\"\"\n",
    "    return [\n",
    "        col for col in df.select_dtypes(include=\"object\").columns\n",
    "        if df[col].nunique() <= max_unique\n",
    "    ]\n",
    "cat_cols = get_categorical_columns(df)\n",
    "category_distribution = {\n",
    "    col: df[col].value_counts(normalize=True).to_dict()\n",
    "    for col in cat_cols\n",
    "}\n",
    "print(cat_cols)\n",
    "print(category_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ece65-02a9-4e17-95b8-25b1fa9260d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
